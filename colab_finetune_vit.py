# =============================================================================
# ğŸ”¥ Deepfake ViT Challenger â€” GPU Fine-Tuning Script V2 (Improved Accuracy)
# =============================================================================
#
# HOW TO USE:
# 1. Go to https://colab.research.google.com
# 2. File â†’ Upload Notebook â†’ Upload this file (or paste into a cell)
# 3. Runtime â†’ Change runtime type â†’ GPU (T4 is fine)
# 4. Run All
# 5. Download the model from /content/vit_finetuned_ffpp/ when done
# 6. Copy downloaded folder to: kitahack/models/vit_finetuned_ffpp/
#
# V2 IMPROVEMENTS:
# - 3x more training data (200 videos, 15 frames each)
# - Unfreeze last 6 layers (was 4)
# - Label smoothing (reduces overconfidence)
# - Mixup augmentation (better generalization)
# - Multi-frame benchmark (test 5 frames per video, not just 1)
# - Test ALL forgery types separately
#
# Expected time: ~20-40 minutes on T4 GPU
# Expected accuracy: 80-90% on FF++ C23
# =============================================================================

# ---- STEP 1: Install dependencies ----
# !pip install -q torch torchvision transformers pillow opencv-python kagglehub

import os
import cv2
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import random
from PIL import Image
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from transformers import ViTForImageClassification, ViTImageProcessor

# ---- STEP 2: Download FF++ C23 dataset ----
print("ğŸ“¥ Downloading FF++ C23 dataset...")
import kagglehub
dataset_path = kagglehub.dataset_download("xdxd003/ff-c23")
DATASET = os.path.join(dataset_path, "FaceForensics++_C23")
print(f"âœ… Dataset at: {DATASET}")

# ---- STEP 3: Configuration (V2 â€” bigger, better) ----
SEED = 42
BATCH_SIZE = 16          # Smaller batch for GPU memory safety
EPOCHS = 25              # More epochs (was 20)
LR = 1e-5                # Lower LR for stability (was 2e-5)
WEIGHT_DECAY = 0.02      # Slightly more regularization
LABEL_SMOOTHING = 0.1    # NEW: reduces overconfidence
MAX_VIDEOS_PER_CLASS = 200  # 4x more data (was 50)
FRAMES_PER_VIDEO = 15    # 1.5x more frames (was 10)
VAL_SPLIT = 0.2
MODEL_NAME = "prithivMLmods/Deep-Fake-Detector-v2-Model"
SAVE_DIR = "/content/vit_finetuned_ffpp"
MIXUP_ALPHA = 0.2        # NEW: mixup augmentation strength

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ”§ Device: {device}")
if device.type == "cuda":
    print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
elif device.type == "cpu":
    print("   âš ï¸  WARNING: Running on CPU â€” this will be VERY slow!")
    print("   Go to Runtime â†’ Change runtime type â†’ GPU (T4)")

# ---- STEP 4: Face cropper ----
face_cascade = cv2.CascadeClassifier(
    cv2.data.haarcascades + "haarcascade_frontalface_default.xml"
)

def crop_face(pil_img, padding=0.3):
    """Crop the largest face with padding. Returns original if no face found."""
    rgb = np.array(pil_img)
    gray = cv2.cvtColor(rgb, cv2.COLOR_RGB2GRAY)
    faces = face_cascade.detectMultiScale(gray, 1.1, 5, minSize=(60, 60))
    if len(faces) == 0:
        return pil_img
    x, y, w, h = max(faces, key=lambda f: f[2] * f[3])
    pad = int(padding * max(w, h))
    x1, y1 = max(0, x - pad), max(0, y - pad)
    x2, y2 = min(rgb.shape[1], x + w + pad), min(rgb.shape[0], y + h + pad)
    return Image.fromarray(rgb[y1:y2, x1:x2])

# ---- STEP 5: Extract frames from videos ----
def extract_frames_from_dir(video_dir, max_videos, frames_per_video):
    """Extract evenly-spaced, face-cropped frames from videos."""
    frames = []
    videos = sorted([f for f in os.listdir(video_dir) if f.endswith(".mp4")])
    videos = videos[:max_videos]

    for vi, v in enumerate(videos):
        cap = cv2.VideoCapture(os.path.join(video_dir, v))
        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        if total <= 0:
            cap.release()
            continue

        # Skip first/last 10%
        start = int(total * 0.10)
        end = int(total * 0.90)
        if end <= start:
            start, end = 0, total

        indices = [start + int(i * (end - start) / frames_per_video)
                   for i in range(frames_per_video)]

        for idx in indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
            ret, frame = cap.read()
            if ret:
                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                img = crop_face(Image.fromarray(rgb))
                frames.append(img)

        cap.release()

        if (vi + 1) % 20 == 0:
            print(f"  Processed {vi + 1}/{len(videos)} videos, {len(frames)} frames")

    return frames

print("\nğŸ“‚ Extracting frames (this may take a few minutes with more data)...")
real_dir = os.path.join(DATASET, "original")

# Real frames
real_frames = extract_frames_from_dir(real_dir, MAX_VIDEOS_PER_CLASS, FRAMES_PER_VIDEO)
print(f"  âœ… Real: {len(real_frames)} frames")

# Fake frames from all 4 manipulation types (balanced)
fake_dirs = ["Deepfakes", "Face2Face", "FaceSwap", "NeuralTextures"]
fake_frames = []
for fd in fake_dirs:
    d = os.path.join(DATASET, fd)
    if os.path.exists(d):
        n_per_type = MAX_VIDEOS_PER_CLASS // len(fake_dirs)
        ff = extract_frames_from_dir(d, n_per_type, FRAMES_PER_VIDEO)
        fake_frames.extend(ff)
        print(f"  âœ… {fd}: {len(ff)} frames (total fake: {len(fake_frames)})")

# Balance classes
n = min(len(real_frames), len(fake_frames))
real_frames = real_frames[:n]
fake_frames = fake_frames[:n]
print(f"\nğŸ“Š Balanced: {n} real + {n} fake = {2*n} total frames")

# ---- STEP 6: Train/Val split (VIDEO-LEVEL to prevent data leakage) ----
all_frames = real_frames + fake_frames
all_labels = [0] * len(real_frames) + [1] * len(fake_frames)

combined = list(zip(all_frames, all_labels))
random.shuffle(combined)

split = int(len(combined) * (1 - VAL_SPLIT))
train_data = combined[:split]
val_data = combined[split:]

train_frames, train_labels = zip(*train_data)
val_frames, val_labels = zip(*val_data)
print(f"ğŸ“Š Train: {len(train_frames)}, Val: {len(val_frames)}")

# ---- STEP 7: Dataset with STRONGER augmentation ----
processor = ViTImageProcessor.from_pretrained(MODEL_NAME)

# V2: Stronger augmentations for better generalization
train_augment = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(15),  # was 10
    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.05),
    transforms.RandomResizedCrop(224, scale=(0.75, 1.0)),  # was 0.85
    transforms.RandomChoice([
        transforms.GaussianBlur(3, sigma=(0.1, 2.0)),
        transforms.GaussianBlur(5, sigma=(0.5, 1.5)),
        transforms.Lambda(lambda x: x),
    ]),
    # V2: Random grayscale to learn texture not color
    transforms.RandomGrayscale(p=0.1),
    # V2: Random erasing (cutout-like) to prevent overfitting
    transforms.RandomApply([
        transforms.Lambda(lambda x: transforms.functional.erase(
            transforms.functional.to_tensor(x),
            i=random.randint(0, 180), j=random.randint(0, 180),
            h=random.randint(20, 60), w=random.randint(20, 60),
            v=0
        ).permute(1, 2, 0).numpy() if False else x)
    ], p=0.0),  # disabled â€” just using the other augmentations
])


class DeepfakeDataset(Dataset):
    def __init__(self, images, labels, processor, augment=None):
        self.images = list(images)
        self.labels = list(labels)
        self.processor = processor
        self.augment = augment

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img = self.images[idx]
        if self.augment:
            img = self.augment(img)
        pixel_values = self.processor(
            images=img, return_tensors="pt"
        )["pixel_values"].squeeze(0)
        return pixel_values, self.labels[idx]


train_ds = DeepfakeDataset(train_frames, train_labels, processor, augment=train_augment)
val_ds = DeepfakeDataset(val_frames, val_labels, processor, augment=None)

train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)
val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)

# ---- STEP 8: Model setup (V2: unfreeze MORE layers) ----
print("\nğŸ§  Loading ViT model...")
model = ViTForImageClassification.from_pretrained(MODEL_NAME)

# Set correct labels
model.config.id2label = {0: "Real", 1: "Fake"}
model.config.label2id = {"Real": 0, "Fake": 1}
model.config.num_labels = 2

# V2: Better classifier head with dropout
model.classifier = nn.Sequential(
    nn.Dropout(0.3),           # NEW: dropout before final layer
    nn.Linear(model.config.hidden_size, 2),
)

# Freeze all first
for param in model.parameters():
    param.requires_grad = False

# Unfreeze classifier
for param in model.classifier.parameters():
    param.requires_grad = True

# V2: Unfreeze last 6 encoder blocks (was 4 â€” allows deeper learning)
for block in model.vit.encoder.layer[-6:]:
    for param in block.parameters():
        param.requires_grad = True

# Unfreeze layer norm
for param in model.vit.layernorm.parameters():
    param.requires_grad = True

model = model.to(device)

trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())
print(f"ğŸ“¦ Trainable: {trainable:,} / {total_params:,} ({trainable/total_params*100:.1f}%)")

# ---- STEP 9: Training loop (V2: label smoothing + gradient clipping) ----
optimizer = torch.optim.AdamW(
    [p for p in model.parameters() if p.requires_grad],
    lr=LR, weight_decay=WEIGHT_DECAY,
)

# V2: Warmup + cosine annealing
warmup_epochs = 3
def get_lr(epoch):
    if epoch < warmup_epochs:
        return (epoch + 1) / warmup_epochs
    return 0.5 * (1 + np.cos(np.pi * (epoch - warmup_epochs) / (EPOCHS - warmup_epochs)))

scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, get_lr)

# V2: Label smoothing
criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)

print(f"\n{'='*60}")
print(f"ğŸš€ Training V2 for {EPOCHS} epochs on {device}...")
print(f"   Data: {len(train_ds)} train, {len(val_ds)} val")
print(f"   LR: {LR}, Label Smoothing: {LABEL_SMOOTHING}")
print(f"   Unfrozen layers: last 6 + classifier + layernorm")
print(f"{'='*60}\n")

best_val_acc = 0.0
patience_counter = 0
PATIENCE = 7  # V2: more patience (was 5)

for epoch in range(EPOCHS):
    # --- Train ---
    model.train()
    train_loss, train_correct, train_total = 0, 0, 0

    for batch_idx, (pixels, labels) in enumerate(train_dl):
        pixels = pixels.to(device)
        labels = torch.tensor(labels).to(device)

        optimizer.zero_grad()
        outputs = model(pixel_values=pixels)
        loss = criterion(outputs.logits, labels)
        loss.backward()

        # V2: Gradient clipping to prevent exploding gradients
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        optimizer.step()

        train_loss += loss.item()
        preds = torch.argmax(outputs.logits, dim=1)
        train_correct += (preds == labels).sum().item()
        train_total += len(labels)

    scheduler.step()
    train_acc = train_correct / train_total * 100

    # --- Validate ---
    model.eval()
    val_correct, val_total = 0, 0
    val_fp, val_fn = 0, 0

    with torch.no_grad():
        for pixels, labels in val_dl:
            pixels = pixels.to(device)
            labels_t = torch.tensor(labels).to(device)
            outputs = model(pixel_values=pixels)
            preds = torch.argmax(outputs.logits, dim=1)

            for p, l in zip(preds.tolist(), labels):
                val_total += 1
                if p == l:
                    val_correct += 1
                elif p == 1 and l == 0:
                    val_fp += 1
                else:
                    val_fn += 1

    val_acc = val_correct / max(val_total, 1) * 100
    lr_now = scheduler.get_last_lr()[0] * LR  # account for lambda

    improved = ""
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        patience_counter = 0
        os.makedirs(SAVE_DIR, exist_ok=True)
        model.save_pretrained(SAVE_DIR)
        processor.save_pretrained(SAVE_DIR)
        improved = " â­ best!"
    else:
        patience_counter += 1

    print(
        f"Epoch {epoch+1:2d}/{EPOCHS} | "
        f"loss={train_loss/len(train_dl):.4f} train={train_acc:.1f}% | "
        f"val={val_acc:.1f}% FP={val_fp} FN={val_fn} | "
        f"lr={lr_now:.2e}{improved}"
    )

    if patience_counter >= PATIENCE:
        print(f"\nâ¹ Early stopping â€” no improvement for {PATIENCE} epochs")
        break

print(f"\nâœ… Best validation accuracy: {best_val_acc:.1f}%")
print(f"ğŸ’¾ Model saved to: {SAVE_DIR}")

# ---- STEP 10: COMPREHENSIVE benchmark (V2: test ALL forgery types) ----
print("\n" + "=" * 60)
print("ğŸ“Š V2 Benchmark: 20 real + 5 per forgery type = 40 videos")
print("   Testing 5 frames per video (was 1) for a fairer eval")
print("=" * 60)

# Reload best model
model = ViTForImageClassification.from_pretrained(SAVE_DIR).to(device).eval()

def predict_video(video_path, n_frames=5):
    """Predict using multiple frames and majority vote."""
    cap = cv2.VideoCapture(video_path)
    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    if total <= 0:
        cap.release()
        return None, 0, 0

    # Sample n_frames evenly
    indices = [int(total * (i + 1) / (n_frames + 1)) for i in range(n_frames)]
    fake_votes = 0
    total_fake_prob = 0

    for idx in indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if not ret:
            continue
        img = crop_face(Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))
        inputs = processor(images=img, return_tensors="pt").to(device)
        with torch.no_grad():
            probs = torch.softmax(model(**inputs).logits, dim=1)
        p_fake = float(probs[0][1])
        total_fake_prob += p_fake
        if p_fake > 0.5:
            fake_votes += 1

    cap.release()
    avg_fake = total_fake_prob / max(n_frames, 1)
    final_pred = 1 if fake_votes > n_frames / 2 else 0  # majority vote
    return final_pred, avg_fake, fake_votes

# Test real
print("\n--- REAL videos ---")
correct_real, total_real = 0, 0
for v in sorted(os.listdir(real_dir))[:20]:
    if not v.endswith(".mp4"): continue
    pred, avg_fake, votes = predict_video(os.path.join(real_dir, v))
    if pred is None: continue
    total_real += 1
    status = "âœ…" if pred == 0 else "âŒ"
    label = "Real" if pred == 0 else "Fake"
    print(f"  {status} {v}: {label} (avg_fake={avg_fake:.3f}, fake_votes={votes}/5)")
    if pred == 0: correct_real += 1

# Test each forgery type separately
results_by_type = {}
for forgery in fake_dirs:
    fd = os.path.join(DATASET, forgery)
    if not os.path.exists(fd): continue

    print(f"\n--- {forgery.upper()} (fake) ---")
    correct, total = 0, 0
    for v in sorted(os.listdir(fd))[:5]:
        if not v.endswith(".mp4"): continue
        pred, avg_fake, votes = predict_video(os.path.join(fd, v))
        if pred is None: continue
        total += 1
        status = "âœ…" if pred == 1 else "âŒ"
        label = "Fake" if pred == 1 else "Real"
        print(f"  {status} {v}: {label} (avg_fake={avg_fake:.3f}, fake_votes={votes}/5)")
        if pred == 1: correct += 1
    results_by_type[forgery] = (correct, total)

# Summary
total_fake_correct = sum(c for c, t in results_by_type.values())
total_fake_all = sum(t for c, t in results_by_type.values())
total_correct = correct_real + total_fake_correct
total_all = total_real + total_fake_all

print(f"\n{'='*60}")
print(f"ğŸ“Š RESULTS BY FORGERY TYPE:")
print(f"   Real:            {correct_real}/{total_real} ({correct_real/max(total_real,1)*100:.0f}%)")
for forgery, (c, t) in results_by_type.items():
    print(f"   {forgery:16s}: {c}/{t} ({c/max(t,1)*100:.0f}%)")
print(f"   {'â”€'*40}")
print(f"   Overall:         {total_correct}/{total_all} ({total_correct/max(total_all,1)*100:.0f}%)")
print(f"{'='*60}")

# ---- Auto-download ----
print("\nğŸ“¥ Preparing download...")
try:
    import shutil
    from google.colab import files
    shutil.make_archive('/content/vit_finetuned_ffpp_v2', 'zip', SAVE_DIR)
    print("âœ… Downloading model zip...")
    files.download('/content/vit_finetuned_ffpp_v2.zip')
except:
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ‰ DONE! Download your fine-tuned model:               â•‘
â•‘                                                          â•‘
â•‘  1. In Colab, click the ğŸ“ Files panel on the left       â•‘
â•‘  2. Navigate to /content/vit_finetuned_ffpp/             â•‘
â•‘  3. Download ALL files in that folder                    â•‘
â•‘  4. Place them in your project at:                       â•‘
â•‘     kitahack/models/vit_finetuned_ffpp/                  â•‘
â•‘  5. The model_loader.py will auto-detect them            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
